## ARCH
### conditional variance VS unconditional variance
unconditional variance can be a constant with conditional variance depending on $x_t$.

conditional variance form
$$y_{t+1}=\epsilon_{t+1}x_t$$

Simultanously, we can use log form of the function above

$$lny_{t+1}=lnx_t+ln\epsilon_{t+1}$$
and
$$var(y_{t+1}|x_t)=var(\epsilon_{t+1}x_t|x_t)$$

## ARCH model

Engel(1982,Econometrica)

Simultanously, model the mean and the variance

$$y_t=a_0+a_1y_{t-1}+\epsilon_t$$

unconditional mean/variance

$$
E(y_t)=\frac{a_0}{1-a_1} \\

var(y_t)=\frac{\sigma^2}{1-\sigma_y^2}
$$

conditional mean/variance

$$
E_t(y_{t+1})=E_t(a_0+a_1y_t+\epsilon_{t+1})=a_0+a_1y_t \\

var(y_{t+1}|y_t,y_{t-1},...)=E_t(y_{t+1}-a_0-a_1y_t)^2=\sigma^2<\frac{\sigma^2}{1-\sigma_y^2}
$$

We can see that conditional variance is smaller than unconditional variance, which means conditional predicting is more prescise.

Engel modeled
$$\epsilon_t=v_t \sqrt{\alpha_0+\alpha_1\epsilon_{t-1}^2} $$


$v_t$:white noise process with $\sigma_v^2=1$

$v_t$ and $\epsilon_{t-1}$ are independent of each other.

#### check {$\epsilon_t$} properties    .

1. Mean of {$\epsilon_t$} is zero.

$$E(\epsilon_t)=E(v_t \sqrt{\alpha_0+\alpha_1\epsilon_{t-1}^2})=0$$

2. $\epsilon_t$ is independent of $\epsilon_{t+i}$

$$E(\epsilon_t \epsilon_{t-i})=E(v_t \sqrt{\alpha_0+\alpha_1\epsilon_{t-1}^2} \times v_{t-i} \sqrt{\alpha_0+\alpha_1\epsilon_{t-i-1}^2})=0$$

3. Variance of {$\epsilon_t$} is a constant.

$$var(\epsilon_t)=var(v_t \sqrt{\alpha_0+\alpha_1\epsilon_{t-1}^2})=E((v_t \sqrt{\alpha_0+\alpha_1\epsilon_{t-1}^2})^2)=\frac{\alpha_0}{1-\alpha_1}$$

So {$\epsilon_t$} is a white noise sequence.

#### conditional mean/Variance of{$\epsilon_t$}

1. $E_{t-1}(\epsilon_t)=E(\epsilon_t|\epsilon_{t-1},\epsilon_{t-2},...)=E(v_t \sqrt{\alpha_0+\alpha_1\epsilon_{t-1}^2}|\epsilon_{t-1})=0$

2. $var(\epsilon_t|epsilon_{t-1})=E(\epsilon_t^2|\epsilon_{t-1})=\alpha_0+\alpha_1 \epsilon_{t-1}^2$
- $\alpha_0 >0$ ensures the variance positive;
- $0<\alpha_1<1$  ensures the sequence stationary.


ARCH $\rightarrow$ autoregressive conditional heteroscedasticity


#### conditional mean/variance of {$y_t$}

1. $E_{t-1}y_t=a_0+a_1y_{t-1}$
2. $var(y_t|y_{t-1})=E_{t-1}((y_t-E_{t-1}y_t)^2)=E_{t-1}(\epsilon_t^2)=\alpha_0+\alpha_1 \epsilon_{t-1}$

##ARCH(q)

$\epsilon_t=v_t\sqrt{\alpha_0+\Sigma_{i=1}^q\alpha_i\epsilon_{t-i}^2}$

##GARCH (Generalized ARCH)

Allow the conditional variance to be an ARMA process
$$
\epsilon_t=v_t\sqrt{h_t} \\

h_t=\alpha_0+\Sigma_{i=1}^q \alpha_i \epsilon_{t-i}^2 +\Sigma_{i=1}^p h_{t-i}
$$

1. When we use ARMA model and get residual sequence, we should test whether the residual sequence is white noise.
2. Then, use PACF & ACF to test whether {$\epsilon_t^2$} is an ARMA process. If the sequence is an ARMA process, we should use arch or garch model.

3. Ljung-Box Q-statistics test {$\epsilon_t^2$} is serially unrelated.


### Mcleod and Li(1983)
Test whether use arch or garch model or not.

1. get {$\epsilon_t^2$}
2. regress
 $\epsilon_t^2=\alpha_0+\alpha_1\epsilon_{t-1}^2+...+\alpha_{q}\epsilon_{t-q}^2$

  $H_0: \alpha_0=...=\alpha_q=0$
### MLE of GARCH

GARCH model

$$y_t=\beta x_t+\epsilon_t\\
\epsilon_t=v_t\sqrt{h_t} \sim N(0,h_t)
\\
h_t=\alpha_0+\alpha_1\epsilon_{t-1}^2
$$
MLE method to estimate garch model.
The joint likelihood of $\epsilon_1$ through $\epsilon_T$

$$L=\Pi_{t=1}^T\frac{1}{\sqrt{2\pi h_t}}exp({\frac{\epsilon_t^2}{2h_t}})$$

$$lnL=-\frac{T}{2}ln(2\pi)-\frac{1}{2}\Sigma_{t=1}^Tln(\alpha_0+\alpha_1\epsilon_{t-i}^2)-1/2\Sigma \frac{\epsilon_t^2}{\alpha_0+\alpha_1\epsilon_t^2}$$

### ARCH-M Model (ARCH-mean)

Engel Lilien and Robins(1987)

allow the mean of a sequence depend on conditional Variance
$y_t=\mu_t+\epsilon_t$
$y_t:$ excess return
